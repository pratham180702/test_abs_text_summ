{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a505326",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\PRATHAM\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe298767",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'first', 'ever', 'iPhone', 'was', 'launched', 'in', '2007', 'by', 'Apple', 'Inc', '.', 'The', 'iPhone', 'revolutionized', 'the', 'mobile', 'phone', 'industry', 'and', 'brought', 'a', 'new', 'level', 'of', 'technology', 'to', 'the', 'market', '.', 'The', 'iPhone', 'had', 'a', 'large', 'impact', 'on', 'the', 'way', 'people', 'use', 'their', 'phones', 'and', 'changed', 'the', 'way', 'people', 'communicate', 'with', 'each', 'other', '.', 'Today', ',', 'there', 'are', 'several', 'different', 'models', 'of', 'iPhones', 'available', 'with', 'even', 'more', 'advanced', 'features', 'and', 'capabilities']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "    \n",
    "article = \"The first ever iPhone was launched in 2007 by Apple Inc. The iPhone revolutionized the mobile phone industry and brought a new level of technology to the market. The iPhone had a large impact on the way people use their phones and changed the way people communicate with each other. Today, there are several different models of iPhones available with even more advanced features and capabilities\"\n",
    "article_tokens = word_tokenize(article)\n",
    "print(article_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6dccac53",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lowering the case :\n",
    "\n",
    "lower_article_tokens = [token.lower() for token in article_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d56500f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove punctuations:\n",
    "\n",
    "import string\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    # create a translation table to remove punctuation and special characters\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    \n",
    "    # remove the punctuation and special characters from the text\n",
    "    cleaned_text = text.translate(table)\n",
    "    \n",
    "    return cleaned_text\n",
    "\n",
    "# remove punctuation and special characters from article_tokens\n",
    "p_removed_article_tokens = [remove_punctuation(token) for token in lower_article_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f246bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing stopwords\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "filtered_article_tokens = [word for word in p_removed_article_tokens if word.lower() not in stop_words]\n",
    "# filtered_highlights_tokens = [word for word in p_removed_highlights_tokens if word.lower() not in stop_words]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "45850283",
   "metadata": {},
   "outputs": [],
   "source": [
    "#stemming\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "article_stemmed_words = [stemmer.stem(word) for word in filtered_article_tokens]\n",
    "# highlights_stemmed_words = [stemmer.stem(word) for word in filtered_article_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "40766951",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "article_lemmatized_words = [lemmatizer.lemmatize(word) for word in article_stemmed_words]\n",
    "# highlights_lemmatized_words = [lemmatizer.lemmatize(word) for word in highlights_stemmed_words]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9bef523c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#WSD implementation\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "def adapted_lesk(word, context, pos=None):\n",
    "    word = word.lower()\n",
    "    context = [w.lower() for w in context]\n",
    "    synsets = wordnet.synsets(word)\n",
    "    if pos is None:\n",
    "        signatures = synsets\n",
    "    else:\n",
    "        signatures = [synset for synset in synsets if synset.pos() == pos]\n",
    "    best_sense = None\n",
    "    max_overlap = 0\n",
    "    for signature in signatures:\n",
    "        definition = signature.definition().split()\n",
    "        definition = [w.lower() for w in definition if w.isalpha()]\n",
    "        definition_synonyms = []\n",
    "        for syn in signature.lemmas():\n",
    "            definition_synonyms.append(syn.name().lower())\n",
    "        definition_synonyms.append(signature.lemma_names()[0].lower())\n",
    "        overlap = set(definition).intersection(context)\n",
    "        if len(overlap) > max_overlap:\n",
    "            max_overlap = len(overlap)\n",
    "            best_sense = signature\n",
    "        if len(overlap) == max_overlap:\n",
    "            definition_len = len(definition)\n",
    "            if best_sense:\n",
    "                best_sense_len = len(best_sense.definition().split())\n",
    "                if definition_len < best_sense_len:\n",
    "                    best_sense = signature\n",
    "            else:\n",
    "                best_sense = signature\n",
    "    return best_sense\n",
    "\n",
    "\n",
    "disambiguated_article_tokens = []\n",
    "for token in article_tokens:\n",
    "    sense = adapted_lesk(token, article_lemmatized_words)\n",
    "    if sense is not None:\n",
    "        disambiguated_article_tokens.append(sense)\n",
    "\n",
    "# disambiguated_highlights_tokens = []\n",
    "# for token in highlights_tokens:\n",
    "#     sense = adapted_lesk(token, highlights_lemmatized_words)\n",
    "#     if sense is not None:\n",
    "#         disambiguated_highlights_tokens.append(sense)\n",
    "\n",
    "article_sense = []\n",
    "for word in article_lemmatized_words:\n",
    "    sense = adapted_lesk(word, article)\n",
    "    if sense is not None:\n",
    "        article_sense.append(sense)\n",
    "    \n",
    "# highlights_sense = []\n",
    "# for word in highlights_lemmatized_words:\n",
    "#     sense = adapted_lesk(word, highlights)\n",
    "#     if sense is not None:\n",
    "#         highlights_sense.append(sense)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f6353653",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('first.n.02'),\n",
       " Synset('ever.r.03'),\n",
       " Synset('launch.v.03'),\n",
       " Synset('iraqi_national_congress.n.01'),\n",
       " Synset('rotation.n.03'),\n",
       " Synset('phone.n.02'),\n",
       " Synset('lend.v.01'),\n",
       " Synset('newfangled.s.01'),\n",
       " Synset('charge.v.17'),\n",
       " Synset('market.v.03'),\n",
       " Synset('impact.n.02'),\n",
       " Synset('way.n.05'),\n",
       " Synset('use.n.03'),\n",
       " Synset('phone.n.02'),\n",
       " Synset('chang_jiang.n.01'),\n",
       " Synset('way.n.05'),\n",
       " Synset('nowadays.r.01'),\n",
       " Synset('discerp.v.02'),\n",
       " Synset('differ.v.01'),\n",
       " Synset('model.n.02'),\n",
       " Synset('avail.n.01'),\n",
       " Synset('tied.s.05')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article_sense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2f05db74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Named Entities: [('ORGANIZATION', 'iPhone'), ('PERSON', 'Apple Inc'), ('ORGANIZATION', 'iPhone'), ('ORGANIZATION', 'iPhone'), ('ORGANIZATION', 'iPhones')]\n",
      "\n",
      "Frequency Distribution: [('the', 4), ('The', 3), ('iPhone', 3), ('.', 3), ('and', 3), ('a', 2), ('of', 2), ('way', 2), ('people', 2), ('with', 2)]\n",
      "\n",
      "Taxonomy: ['The', 'first', 'ever', 'iPhone', 'be', 'launch', 'in', '2007', 'by', 'Apple', 'Inc', '.', 'The', 'iPhone', 'revolutionize', 'the', 'mobile', 'phone', 'industry', 'and', 'bring', 'a', 'new', 'level', 'of', 'technology', 'to', 'the', 'market', '.', 'The', 'iPhone', 'have', 'a', 'large', 'impact', 'on', 'the', 'way', 'people', 'use', 'their', 'phone', 'and', 'change', 'the', 'way', 'people', 'communicate', 'with', 'each', 'other', '.', 'Today', ',', 'there', 'be', 'several', 'different', 'model', 'of', 'iPhones', 'available', 'with', 'even', 'more', 'advanced', 'feature', 'and', 'capability']\n",
      "\n",
      "θd: {'The': set(), 'first': {'beginning', 'first', 'inaugural'}, 'ever': {'ever', 'always'}, 'iPhone': set(), 'was': {'cost', 'embody', 'constitute', 'be', 'exist', 'equal'}, 'launched': {'establish', 'launch', 'plunge'}, 'in': {'inch', 'indium', 'Indiana'}, '2007': set(), 'by': set(), 'Apple': {'apple'}, 'Inc': {'Iraqi_National_Congress'}, '.': set(), 'revolutionized': {'revolutionize'}, 'the': set(), 'mobile': {'mobile', 'fluid'}, 'phone': {'earphone', 'phone', 'telephone'}, 'industry': {'diligence', 'industry'}, 'and': set(), 'brought': {'lend', 'institute', 'fetch', 'bring'}, 'a': {'ampere', 'deoxyadenosine_monophosphate', 'vitamin_A', 'A', 'angstrom', 'adenine'}, 'new': {'Modern', 'New', 'fresh', 'new', 'newfangled', 'raw'}, 'level': {'floor', 'degree', 'level', 'grade', 'horizontal_surface'}, 'of': set(), 'technology': {'technology', 'engineering'}, 'to': set(), 'market': {'marketplace', 'grocery_store', 'market'}, 'had': {'induce', 'have', 'suffer', 'own', 'take', 'hold', 'consume', 'receive', 'give_birth', 'get', 'experience', 'accept'}, 'large': {'large', 'big', 'bombastic'}, 'impact': {'shock', 'impact', 'impingement'}, 'on': set(), 'way': {'direction', 'room', 'way', 'means', 'manner'}, 'people': {'people', 'multitude', 'citizenry'}, 'use': {'practice', 'use'}, 'their': set(), 'phones': {'earphone', 'phone', 'telephone'}, 'changed': {'change', 'exchange', 'deepen', 'switch', 'transfer'}, 'communicate': {'commune', 'communicate', 'convey'}, 'with': set(), 'each': set(), 'other': {'early', 'other'}, 'Today': {'today'}, ',': set(), 'there': {'there'}, 'are': {'cost', 'embody', 'constitute', 'be', 'exist', 'equal'}, 'several': {'respective', 'several'}, 'different': {'different', 'unlike'}, 'models': {'exemplar', 'model', 'mannequin'}, 'iPhones': set(), 'available': {'available'}, 'even': {'even'}, 'more': {'more'}, 'advanced': {'advance', 'advanced'}, 'features': {'feature', 'feature_of_speech'}, 'capabilities': {'capability'}}\n",
      "\n",
      "θf: {'The': 0.04285714285714286, 'first': 0.014285714285714285, 'ever': 0.014285714285714285, 'iPhone': 0.04285714285714286, 'was': 0.014285714285714285, 'launched': 0.014285714285714285, 'in': 0.014285714285714285, '2007': 0.014285714285714285, 'by': 0.014285714285714285, 'Apple': 0.014285714285714285, 'Inc': 0.014285714285714285, '.': 0.04285714285714286, 'revolutionized': 0.014285714285714285, 'the': 0.05714285714285714, 'mobile': 0.014285714285714285, 'phone': 0.014285714285714285, 'industry': 0.014285714285714285, 'and': 0.04285714285714286, 'brought': 0.014285714285714285, 'a': 0.02857142857142857, 'new': 0.014285714285714285, 'level': 0.014285714285714285, 'of': 0.02857142857142857, 'technology': 0.014285714285714285, 'to': 0.014285714285714285, 'market': 0.014285714285714285, 'had': 0.014285714285714285, 'large': 0.014285714285714285, 'impact': 0.014285714285714285, 'on': 0.014285714285714285, 'way': 0.02857142857142857, 'people': 0.02857142857142857, 'use': 0.014285714285714285, 'their': 0.014285714285714285, 'phones': 0.014285714285714285, 'changed': 0.014285714285714285, 'communicate': 0.014285714285714285, 'with': 0.02857142857142857, 'each': 0.014285714285714285, 'other': 0.014285714285714285, 'Today': 0.014285714285714285, ',': 0.014285714285714285, 'there': 0.014285714285714285, 'are': 0.014285714285714285, 'several': 0.014285714285714285, 'different': 0.014285714285714285, 'models': 0.014285714285714285, 'iPhones': 0.014285714285714285, 'available': 0.014285714285714285, 'even': 0.014285714285714285, 'more': 0.014285714285714285, 'advanced': 0.014285714285714285, 'features': 0.014285714285714285, 'capabilities': 0.014285714285714285}\n"
     ]
    }
   ],
   "source": [
    "# FOR ARTICLE\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "def get_wordnet_pos(pos_tag):\n",
    "    if pos_tag[1].startswith('J'):\n",
    "        return (pos_tag[0], wn.ADJ)\n",
    "    elif pos_tag[1].startswith('V'):\n",
    "        return (pos_tag[0], wn.VERB)\n",
    "    elif pos_tag[1].startswith('N'):\n",
    "        return (pos_tag[0], wn.NOUN)\n",
    "    elif pos_tag[1].startswith('R'):\n",
    "        return (pos_tag[0], wn.ADV)\n",
    "    else:\n",
    "        return (pos_tag[0], wn.NOUN)\n",
    "\n",
    "def preprocess(article):\n",
    "    # Named Entity Recognition\n",
    "    entities = nltk.ne_chunk(nltk.pos_tag(word_tokenize(article)))\n",
    "    entity_list = []\n",
    "    for entity in entities:\n",
    "        if hasattr(entity, 'label'):\n",
    "            entity_list.append((entity.label(), \" \".join(c[0] for c in entity.leaves())))\n",
    "    # Frequency Distribution\n",
    "    words = nltk.word_tokenize(article)\n",
    "    fdist = nltk.FreqDist(words)\n",
    "    # Taxonomy\n",
    "    tagged_words = pos_tag(words)\n",
    "    wnl = WordNetLemmatizer()\n",
    "    lemmatized_words = [wnl.lemmatize(*get_wordnet_pos(tagged_word)) for tagged_word in tagged_words]\n",
    "    # θd\n",
    "    theta_d = {}\n",
    "    for word, pos in tagged_words:\n",
    "        synonyms = wn.synsets(word, pos=wn.NOUN if pos.startswith(\"N\") else (wn.ADJ if pos.startswith(\"J\") else (wn.VERB if pos.startswith(\"V\") else (wn.ADV if pos.startswith(\"R\") else wn.NOUN))))\n",
    "        theta_d[word] = set(syn.lemmas()[0].name() for syn in synonyms)\n",
    "    # θf\n",
    "    theta_f = {}\n",
    "    for word, pos in tagged_words:\n",
    "        theta_f[word] = fdist[word] / float(len(words))\n",
    "    return (entities, entity_list, fdist, lemmatized_words, theta_d, theta_f)\n",
    "\n",
    "\n",
    "article_data = preprocess(article)\n",
    "\n",
    "\n",
    "named_entities = article_data[1]\n",
    "Frequency_Distribution = article_data[2].most_common(10)\n",
    "Taxonomy = article_data[3]\n",
    "fi_d = article_data[4]\n",
    "fi_f = article_data[5]\n",
    "\n",
    "# Print Named Entities\n",
    "print(\"Named Entities:\", article_data[1])\n",
    "# Print Frequency Distribution\n",
    "print(\"\\nFrequency Distribution:\", article_data[2].most_common(10))\n",
    "# Print Taxonomy\n",
    "print(\"\\nTaxonomy:\", article_data[3])\n",
    "# Print θd\n",
    "print(\"\\nθd:\", article_data[4])\n",
    "# Print θf\n",
    "print(\"\\nθf:\", article_data[5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dc67e4f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 4),\n",
       " ('The', 3),\n",
       " ('iPhone', 3),\n",
       " ('.', 3),\n",
       " ('and', 3),\n",
       " ('a', 2),\n",
       " ('of', 2),\n",
       " ('way', 2),\n",
       " ('people', 2),\n",
       " ('with', 2)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Frequency_Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "41c4835f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entity move move entity entity entity chentitynge tiedfentityngled move entityct entity entity entity entity entity entity nowentitydentityys chentityngedustry differ entity entity tied level of technology to entity mentityrket. entity entity hentityd entity lentityrge impentityct on entity wentityy people use entityir nowentitydentityyss differ chentitynged entity wentityy people communicentityte with eentitych oentityr. Todentityy, entityre entityre smoveentityl different models of entitys entityventityilentityble with even more entitydventitynced feentitytures differ centitypentitybilities\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PRATHAM\\AppData\\Local\\Temp\\ipykernel_12824\\1076467673.py:16: DeprecationWarning: \n",
      "  Function _synset_from_pos_and_offset() has been deprecated.  Use\n",
      "  public method synset_from_pos_and_offset() instead\n",
      "  synsets = wordnet._synset_from_pos_and_offset(c.pos(), c.offset())\n"
     ]
    }
   ],
   "source": [
    "text = article\n",
    "E = named_entities\n",
    "F = Frequency_Distribution\n",
    "T = Taxonomy\n",
    "θf = 2\n",
    "θd = 3\n",
    "\n",
    "\n",
    "def Level_driven_text_Generalization(text, wsdText, F, T, θd, θf):\n",
    "    genText = text\n",
    "    tokens = text.split()\n",
    "    for i, token in enumerate(tokens):\n",
    "        if i >= len(wsdText):\n",
    "            break\n",
    "        c = wsdText[i]\n",
    "        synsets = wordnet._synset_from_pos_and_offset(c.pos(), c.offset())\n",
    "        Pc = synsets.hypernym_paths()\n",
    "        dc = synsets.max_depth()\n",
    "        fc = next((f[1] for f in F if f[0] == synsets.lemmas()[0].name()), 0)\n",
    "        for path in Pc:\n",
    "            for c in path:\n",
    "                dc = c.max_depth()\n",
    "                fc = next((f[1] for f in F if f[0] == c.lemmas()[0].name()), 0)\n",
    "                if fc >= θf or dc <= θd:\n",
    "                    break\n",
    "            if fc >= θf or dc <= θd:\n",
    "                break\n",
    "        if c.lemmas()[0].name() != token:\n",
    "            genText = genText.replace(token, c.lemmas()[0].name())\n",
    "            F = [(f[0], f[1]-1) if f[0] == token else f for f in F]\n",
    "            F = [(f[0], f[1]+1) if f[0] == c.lemmas()[0].name() else f for f in F]\n",
    "    return genText\n",
    "\n",
    "def Named_entities_driven_text_Generalization(text, E, F, θf):\n",
    "    genText = text\n",
    "    for namedEntity, entity in E:\n",
    "        if next((f[1] for f in F if f[0] == entity), 0) < θf:\n",
    "            genText = genText.replace(namedEntity, entity)\n",
    "    return genText\n",
    "\n",
    "def WSD_Based_Combination_of_NEG_and_LG(text, wsdText, E, F, T, θf, θd):\n",
    "    genText = Named_entities_driven_text_Generalization(text, E, F, θf)\n",
    "    genText = Level_driven_text_Generalization(genText, wsdText, F, T, θd, θf)\n",
    "    return genText\n",
    "\n",
    "result = WSD_Based_Combination_of_NEG_and_LG(text, article_sense, E, F, T, θf, θd)\n",
    "print(result)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
